{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating surprisal from language models\n",
    "Take sentences from CommitmentBank, MegaAttitudes, and stimuli from experiment, mask the attitude predicate, and get predicted probability of occurrence for the target verb. Then, calculate from that the surprisal of the verb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This makes the display show more info\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "1. [Read in the three datasets](#Read-in-the-three-datasets)\n",
    "2. [Masking out the correct verb](#Masking-out-the-correct-verb)\n",
    "    1. [Remaining cases](#Remaining-cases)\n",
    "    2. [Proposed Solution](#Proposed-Solution)\n",
    "        1. [Step 1. Create a new column with list of pos tagged verbs from Sentence](#Step-1.-Create-a-new-column-with-list-of-pos-tagged-verbs-from-Sentence)\n",
    "        2. [Step 2. Lemmatize VerbList](#Step-2.-Lemmatize-VerbList)\n",
    "        3. [TROUBLESHOOT NEEDED](#TROUBLESHOOT-NEEDED)\n",
    "    3. [Combine the datafriends together again](#Combine-the-dataframes-together-again)\n",
    "    4. [Mask out the VerbToken from Sentence](#Mask-out-the-VerbToken-from-Sentence)\n",
    "4. [Masked language modeling to estimate surprisal](#Masked-language-modeling-to-estimate-surprisal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the three datasets\n",
    "- Subset the dfs to just the relevant columns: ID, Verb, Sentence\n",
    "- Make sure that the column names are consistent across the tree dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CommitmentBank\n",
    "# raw url: https://raw.githubusercontent.com/khuyen-le/projectivity-factors/master/data/CommitmentBank-All.csv\n",
    "cb = pd.read_csv(\"../data/CommitmentBank-ALL.csv\")[[\"uID\",\"Verb\",\"Target\"]].drop_duplicates()\n",
    "cb = cb.rename(columns={\"Target\": \"Sentence\",\"uID\":\"ID\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MegaVeridicality\n",
    "# raw URL: https://raw.githubusercontent.com/khuyen-le/projectivity-factors/master/data/mega-veridicality-v2.csv\n",
    "mv = pd.read_csv(\"../data/mega-veridicality-v2.csv\")[[\"verb\",\"frame\",\"voice\",\"sentence\"]].drop_duplicates()\n",
    "mv = mv.rename(columns={\"verb\": \"Verb\", \"sentence\":\"Sentence\"})\n",
    "mv[\"ID\"] = mv[['frame', 'voice']].apply(lambda x: '_'.join(x), axis=1)\n",
    "mv = mv.drop(columns=[\"frame\",\"voice\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arousal/Valence Study\n",
    "# raw URL: https://raw.githubusercontent.com/khuyen-le/projectivity-factors/master/data/1_sliderprojection/exp1_test-trials.csv\n",
    "vs = pd.read_csv(\"../data/1_sliderprojection/exp1_test-trials.csv\")[[\"Word\",\"utterance\",\"exp\"]]\n",
    "vs = vs[vs[\"exp\"]==\"stim\"].drop_duplicates().drop(columns={\"exp\"})\n",
    "vs = vs.rename(columns={\"Word\": \"Verb\",\"utterance\":\"Sentence\"})\n",
    "vs[\"ID\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine them together into one df\n",
    "df = pd.concat([cb,mv,vs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Verb</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BNC-1</td>\n",
       "      <td>admit</td>\n",
       "      <td>They were still close enough to shore for him to return her to the police if she admitted she was not an experienced ocean sailor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BNC-1002</td>\n",
       "      <td>say</td>\n",
       "      <td>Indeed it could be said that they had prospered.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BNC-1003</td>\n",
       "      <td>say</td>\n",
       "      <td>He might have said to her that some time in the middle of the nineteenth century a cult had grown up around the idea of the home.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>BNC-1005</td>\n",
       "      <td>say</td>\n",
       "      <td>Of course she could say it was for the children as people always did... It was true up to a point.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>BNC-1006</td>\n",
       "      <td>say</td>\n",
       "      <td>Robyn swallowed and took a deep breath trying to compose herself so that when he returned she could say that it was all right she felt fine now.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID   Verb  \\\n",
       "0      BNC-1  admit   \n",
       "9   BNC-1002    say   \n",
       "17  BNC-1003    say   \n",
       "29  BNC-1005    say   \n",
       "37  BNC-1006    say   \n",
       "\n",
       "                                                                                                                                            Sentence  \n",
       "0                 They were still close enough to shore for him to return her to the police if she admitted she was not an experienced ocean sailor.  \n",
       "9                                                                                                   Indeed it could be said that they had prospered.  \n",
       "17                 He might have said to her that some time in the middle of the nineteenth century a cult had grown up around the idea of the home.  \n",
       "29                                                Of course she could say it was for the children as people always did... It was true up to a point.  \n",
       "37  Robyn swallowed and took a deep breath trying to compose herself so that when he returned she could say that it was all right she felt fine now.  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the correct verb token\n",
    "What we need to do is mask out the correct verb in each of the sentences. We have the correct verb in the Verb column. We can easily use apply() with str.replace() to switch the verb with [MASK]. The problem is that the verbs in the sentences are inflected tokens, while the verbs in Verb are lemmatized.\n",
    "\n",
    "\n",
    "For some of the verbs, we don't need to worry about this problem because there is morphological overlap between the Verb Token and the Verb Lemma. \n",
    "\n",
    "\n",
    "Solution:\n",
    "1. Create a new verb token column\n",
    "2. Regex + literal string interpolation to match works in cases where the Verb matches morphologically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frustratingly, this isn't working\n",
    "# df[\"VerbToken\"] = df['Sentence'].str.extract(fr'({df[\"Verb\"]}\\w*)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find a match in the Sentence column for the verb from the Verb column using a regex re.search() returns a match object, so you have to call .group() to get the string that is matched. In cases where there is no match, a NoneType object is returned and you can't call .group() on that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"VerbToken\"] = df.apply(lambda x: re.search(fr'({x[\"Verb\"]}\\w*)',x['Sentence']), axis=1)\n",
    "\n",
    "# In some cases there is nothing captured, it returns a NoneType and causes the code to fail\n",
    "# because NoneType has no method .group()\n",
    "df[\"VerbToken\"] = df[\"VerbToken\"].apply(lambda x: x.group() if x is not None else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Verb</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>VerbToken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BNC-1</td>\n",
       "      <td>admit</td>\n",
       "      <td>They were still close enough to shore for him to return her to the police if she admitted she was not an experienced ocean sailor.</td>\n",
       "      <td>admitted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BNC-1002</td>\n",
       "      <td>say</td>\n",
       "      <td>Indeed it could be said that they had prospered.</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BNC-1003</td>\n",
       "      <td>say</td>\n",
       "      <td>He might have said to her that some time in the middle of the nineteenth century a cult had grown up around the idea of the home.</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>BNC-1005</td>\n",
       "      <td>say</td>\n",
       "      <td>Of course she could say it was for the children as people always did... It was true up to a point.</td>\n",
       "      <td>say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>BNC-1006</td>\n",
       "      <td>say</td>\n",
       "      <td>Robyn swallowed and took a deep breath trying to compose herself so that when he returned she could say that it was all right she felt fine now.</td>\n",
       "      <td>say</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID   Verb  \\\n",
       "0      BNC-1  admit   \n",
       "9   BNC-1002    say   \n",
       "17  BNC-1003    say   \n",
       "29  BNC-1005    say   \n",
       "37  BNC-1006    say   \n",
       "\n",
       "                                                                                                                                            Sentence  \\\n",
       "0                 They were still close enough to shore for him to return her to the police if she admitted she was not an experienced ocean sailor.   \n",
       "9                                                                                                   Indeed it could be said that they had prospered.   \n",
       "17                 He might have said to her that some time in the middle of the nineteenth century a cult had grown up around the idea of the home.   \n",
       "29                                                Of course she could say it was for the children as people always did... It was true up to a point.   \n",
       "37  Robyn swallowed and took a deep breath trying to compose herself so that when he returned she could say that it was all right she felt fine now.   \n",
       "\n",
       "   VerbToken  \n",
       "0   admitted  \n",
       "9       None  \n",
       "17      None  \n",
       "29       say  \n",
       "37       say  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remaining cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cases where the above solution did not work\n",
    "empty = df[df[\"VerbToken\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.060509554140127"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(empty)/len(df)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed Solution\n",
    "Overarching: lemmatize Sentence, find the verb lemma that matches the respective Verb column. But we actually need the actual verb token not the lemma, because to replace the correct verb in Sentence with [Mask], we will need to extract the relevant token in order to do a successful str.replace().\n",
    "\n",
    "More concrete:\n",
    "1. Make a new column with POS tag verbs from Sentence\n",
    "2. Lemmatize the verbs from the new column\n",
    "3. Here there be dragons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from: https://gaurav5430.medium.com/using-nltk-for-lemmatizing-sentences-c1bfff963258\n",
    "\n",
    "# initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "# def lemmatize_sentence(sentence):\n",
    "#     #tokenize the sentence and find the POS tag for each token\n",
    "#     nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "#     #tuple of (token, wordnet_tag)\n",
    "#     wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "#     lemmatized_sentence = []\n",
    "#     for word, tag in wordnet_tagged:\n",
    "#         if tag is None:\n",
    "#             #if there is no available tag, append the token as is\n",
    "#             lemmatized_sentence.append(word)\n",
    "#         else:        \n",
    "#             #else use the tag to lemmatize the token\n",
    "#             lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "#     return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Failed code attempt to do it all in one\n",
    "\n",
    "# ```\n",
    "def lemmatize_verb_from_sentence(sentence,verb):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lw = []\n",
    "#     for i in range(0,len(empty)-1):\n",
    "#     for v in empty[\"Verb\"].values:\n",
    "#         verb_from_empty = empty[\"Verb\"].values[i]\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            continue\n",
    "        elif tag != 'v':\n",
    "            continue\n",
    "        else:\n",
    "            lemma = lemmatizer.lemmatize(word, tag)\n",
    "            if lemma != verb:\n",
    "                # Go to the next word/tag pair to find the relevant verb\n",
    "                break\n",
    "            elif lemma == verb:\n",
    "#                 print(\"{verb}: {word} {lemma}\".format(verb=verb,word=word,lemma=lemma))\n",
    "                lw.append(word)\n",
    "                lw.append(lemma)\n",
    "#     print(lw)\n",
    "    return ' '.join(lw)\n",
    "# ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n",
      "say: said say\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-80084fc657e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mverb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Verb\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Sentence\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlemmatize_verb_from_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3848\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-80084fc657e7>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mverb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Verb\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Sentence\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlemmatize_verb_from_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-3db2de998e8e>\u001b[0m in \u001b[0;36mlemmatize_verb_from_sentence\u001b[0;34m(sentence, verb)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlemmatize_verb_from_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#tokenize the sentence and find the POS tag for each token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mnltk_tagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m#tuple of (token, wordnet_tag)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mwordnet_tagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnltk_tag_to_wordnet_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnltk_tagged\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \"\"\"\n\u001b[1;32m    160\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_pos_tag\u001b[0;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[1;32m    116\u001b[0m         )\n\u001b[1;32m    117\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtagged_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Maps to the specified tagset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"eng\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, tokens, return_conf, use_tagdict)\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                 \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_conf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_conf\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, features, return_conf)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# Do a secondary alphabetic sort, for stability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "l = []\n",
    "for verb in df[\"Verb\"].iterrows():\n",
    "    l.append(empty[\"Sentence\"].apply(lambda x: lemmatize_verb_from_sentence(x,verb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Create a new column with list of pos tagged verbs from Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verb(sentence):\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "    verbs = []\n",
    "    for i in nltk_tagged:\n",
    "        if 'VB' in i[1]:\n",
    "            verbs.append(i)\n",
    "    return verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-463aecabe8aa>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  empty[\"VerbList\"] = empty[\"Sentence\"].apply(lambda x: get_verb(x))\n"
     ]
    }
   ],
   "source": [
    "empty[\"VerbList\"] = empty[\"Sentence\"].apply(lambda x: get_verb(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is possibly not necessary\n",
    "# empty[\"VerbTagged\"] = empty[\"Verb\"].apply(lambda x: nltk.pos_tag([x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Lemmatize VerbList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_from_nltk_tagged_list(nltk_tagged):\n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-8a1491a9a5a8>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  empty[\"VerbListLemmatized\"] = empty[\"VerbList\"].apply(lambda x: lemmatize_from_nltk_tagged_list(x))\n"
     ]
    }
   ],
   "source": [
    "empty[\"VerbListLemmatized\"] = empty[\"VerbList\"].apply(lambda x: lemmatize_from_nltk_tagged_list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Verb</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>VerbToken</th>\n",
       "      <th>VerbList</th>\n",
       "      <th>VerbListLemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BNC-1002</td>\n",
       "      <td>say</td>\n",
       "      <td>Indeed it could be said that they had prospered.</td>\n",
       "      <td>None</td>\n",
       "      <td>[(be, VB), (said, VBD), (had, VBD), (prospered...</td>\n",
       "      <td>[be, say, have, prosper]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BNC-1003</td>\n",
       "      <td>say</td>\n",
       "      <td>He might have said to her that some time in th...</td>\n",
       "      <td>None</td>\n",
       "      <td>[(have, VB), (said, VBD), (had, VBD), (grown, ...</td>\n",
       "      <td>[have, say, have, grow]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>BNC-1145</td>\n",
       "      <td>tell</td>\n",
       "      <td>She could also have told this was Tina's mothe...</td>\n",
       "      <td>None</td>\n",
       "      <td>[(have, VB), (told, VBN), (was, VBD), (went, V...</td>\n",
       "      <td>[have, tell, be, go, lead]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>BNC-1187</td>\n",
       "      <td>think</td>\n",
       "      <td>They may have thought they were putting it out...</td>\n",
       "      <td>None</td>\n",
       "      <td>[(have, VB), (thought, VBN), (were, VBD), (put...</td>\n",
       "      <td>[have, think, be, put, beautify]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>BNC-1194</td>\n",
       "      <td>think</td>\n",
       "      <td>Perhaps he thought that her own wishes would h...</td>\n",
       "      <td>None</td>\n",
       "      <td>[(thought, VBD), (be, VB), (considered, VBN)]</td>\n",
       "      <td>[think, be, consider]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID   Verb                                           Sentence  \\\n",
       "9    BNC-1002    say   Indeed it could be said that they had prospered.   \n",
       "17   BNC-1003    say  He might have said to her that some time in th...   \n",
       "575  BNC-1145   tell  She could also have told this was Tina's mothe...   \n",
       "716  BNC-1187  think  They may have thought they were putting it out...   \n",
       "733  BNC-1194  think  Perhaps he thought that her own wishes would h...   \n",
       "\n",
       "    VerbToken                                           VerbList  \\\n",
       "9        None  [(be, VB), (said, VBD), (had, VBD), (prospered...   \n",
       "17       None  [(have, VB), (said, VBD), (had, VBD), (grown, ...   \n",
       "575      None  [(have, VB), (told, VBN), (was, VBD), (went, V...   \n",
       "716      None  [(have, VB), (thought, VBN), (were, VBD), (put...   \n",
       "733      None      [(thought, VBD), (be, VB), (considered, VBN)]   \n",
       "\n",
       "                   VerbListLemmatized  \n",
       "9            [be, say, have, prosper]  \n",
       "17            [have, say, have, grow]  \n",
       "575        [have, tell, be, go, lead]  \n",
       "716  [have, think, be, put, beautify]  \n",
       "733             [think, be, consider]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-714684832097>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  empty[\"VerbListLemmatizedTagged\"] = empty[\"VerbListLemmatized\"].apply(lambda x: nltk.pos_tag(x))\n"
     ]
    }
   ],
   "source": [
    "empty[\"VerbListLemmatizedTagged\"] = empty[\"VerbListLemmatized\"].apply(lambda x: nltk.pos_tag(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TROUBLESHOOT NEEDED\n",
    "what i want to do is: \n",
    "1. pair each element of the lists together so that each token and it's corresponding lemma are together (zip() should be able to do that, i think we would get a list of lists)\n",
    "2. Then it should be easy to search through the list using indexing for a match with the corresponding verb from Verb on the verb lemma. the match should allow us to return the correct Token,Lemma list\n",
    "3. Once we have that list, there are several solutions, to just index into it to get the VerbToken that matches the verb lemma in the Verb column\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9                   say\n",
       "17                  say\n",
       "575                tell\n",
       "716               think\n",
       "733               think\n",
       "782               think\n",
       "791               think\n",
       "799               think\n",
       "817               think\n",
       "825               think\n",
       "850               think\n",
       "859               think\n",
       "893               think\n",
       "901               think\n",
       "927               think\n",
       "936               think\n",
       "945               think\n",
       "954               think\n",
       "963               think\n",
       "971                know\n",
       "979               think\n",
       "988               think\n",
       "997               think\n",
       "1005              think\n",
       "1146               feel\n",
       "1197               know\n",
       "1222             forget\n",
       "1232             forget\n",
       "1259             forget\n",
       "1267             forget\n",
       "1657               know\n",
       "1681               know\n",
       "1833               know\n",
       "2043            realize\n",
       "2051            realize\n",
       "2059            realize\n",
       "2077            realize\n",
       "2136               know\n",
       "2145                say\n",
       "2162                say\n",
       "2204               know\n",
       "2405               tell\n",
       "2422               tell\n",
       "2446              think\n",
       "2463              think\n",
       "2480              think\n",
       "2488              think\n",
       "2554              think\n",
       "2596              think\n",
       "2604              think\n",
       "2917             forget\n",
       "2925             forget\n",
       "2968               know\n",
       "3038               know\n",
       "3046               know\n",
       "3054               know\n",
       "3258            realize\n",
       "3290                say\n",
       "3299                say\n",
       "3307                say\n",
       "3364                say\n",
       "3440               tell\n",
       "3490              think\n",
       "3523                say\n",
       "3531                say\n",
       "3549                say\n",
       "3557                say\n",
       "3569                say\n",
       "3577                say\n",
       "3588                say\n",
       "3646                say\n",
       "3800               tell\n",
       "3809               tell\n",
       "3817               tell\n",
       "3826               tell\n",
       "3835               tell\n",
       "3843               tell\n",
       "3851               tell\n",
       "3860               tell\n",
       "3868              think\n",
       "3881              think\n",
       "3891              think\n",
       "3899              think\n",
       "3907              think\n",
       "3915              think\n",
       "3954              think\n",
       "3993              think\n",
       "4001              think\n",
       "4010              think\n",
       "4018              think\n",
       "4277               feel\n",
       "4311               feel\n",
       "4430               feel\n",
       "4455               feel\n",
       "4479               feel\n",
       "4496               find\n",
       "4522             forget\n",
       "4547             forget\n",
       "4711               hope\n",
       "4933               know\n",
       "4941               know\n",
       "4975               know\n",
       "5012               find\n",
       "5046               find\n",
       "5055               find\n",
       "5288            realize\n",
       "5340                say\n",
       "5348                say\n",
       "5374                say\n",
       "5383                say\n",
       "5411                say\n",
       "5899              think\n",
       "6632              think\n",
       "7546              think\n",
       "7870              think\n",
       "8186                say\n",
       "8290                see\n",
       "9462            foresee\n",
       "9666               know\n",
       "10394             think\n",
       "10827             think\n",
       "10941              hope\n",
       "11068               say\n",
       "11180            forget\n",
       "11351              feel\n",
       "17                 deny\n",
       "21             find_out\n",
       "25             flip_out\n",
       "34              forgive\n",
       "45              terrify\n",
       "51              clarify\n",
       "66                 weep\n",
       "688            come_out\n",
       "692             gratify\n",
       "696            prophesy\n",
       "728               worry\n",
       "743             satisfy\n",
       "1374            mystify\n",
       "1377         figure_out\n",
       "1385         dissatisfy\n",
       "1406           foretell\n",
       "2051            horrify\n",
       "2058               envy\n",
       "2080              curse\n",
       "2093           flip_out\n",
       "2098                say\n",
       "2101            hush_up\n",
       "2107                cry\n",
       "2725            certify\n",
       "2731            testify\n",
       "2735         reason_out\n",
       "2737     piece_together\n",
       "2749            hush_up\n",
       "2757            fess_up\n",
       "2768          freak_out\n",
       "2779               tell\n",
       "2782           foretell\n",
       "2787            horrify\n",
       "3400          freak_out\n",
       "3414          spellbind\n",
       "3421           flip_out\n",
       "3440           come_out\n",
       "3454            satisfy\n",
       "4094         figure_out\n",
       "4100            gratify\n",
       "4117            specify\n",
       "4138           find_out\n",
       "4147            stupefy\n",
       "4770             notify\n",
       "4771              swear\n",
       "4774            fess_up\n",
       "4805           foretell\n",
       "4806          freak_out\n",
       "5448              imply\n",
       "5489            petrify\n",
       "5494            mislead\n",
       "5501             uphold\n",
       "5506          freak_out\n",
       "6133         reason_out\n",
       "6148          point_out\n",
       "6162               feel\n",
       "6808            signify\n",
       "6817             notify\n",
       "6827              worry\n",
       "6838            petrify\n",
       "6846            stupefy\n",
       "7484            mislead\n",
       "7486          spellbind\n",
       "7510            foresee\n",
       "7514            mortify\n",
       "7516     piece_together\n",
       "7526            mortify\n",
       "7538               tell\n",
       "7547               pity\n",
       "8163              think\n",
       "8172              reply\n",
       "8173            mystify\n",
       "8185          point_out\n",
       "8194              write\n",
       "8205           flip_out\n",
       "8206            terrify\n",
       "8881              worry\n",
       "8903                see\n",
       "9530               know\n",
       "9537           identify\n",
       "9539         dissatisfy\n",
       "9553               sing\n",
       "9565             verify\n",
       "10205        understand\n",
       "10225            forget\n",
       "10253              find\n",
       "10893              sing\n",
       "10895           hush_up\n",
       "10928        dissatisfy\n",
       "10934        figure_out\n",
       "10936           petrify\n",
       "11577             worry\n",
       "11615           mislead\n",
       "11627           satisfy\n",
       "12243           stupefy\n",
       "12244         point_out\n",
       "12257            uphold\n",
       "12274           certify\n",
       "12288           hush_up\n",
       "12298         spellbind\n",
       "12305         freak_out\n",
       "12927           gratify\n",
       "12929             imply\n",
       "12976              tell\n",
       "12984          foretell\n",
       "13601           mystify\n",
       "13604              find\n",
       "13620        understand\n",
       "13625           petrify\n",
       "13641              feel\n",
       "13645         freak_out\n",
       "13647           mortify\n",
       "13650          flip_out\n",
       "13653          find_out\n",
       "13662           terrify\n",
       "14280             think\n",
       "14302           fess_up\n",
       "14308          foretell\n",
       "14314               cry\n",
       "14329             curse\n",
       "14337        reason_out\n",
       "14338            forget\n",
       "14966           forgive\n",
       "14970           foresee\n",
       "14974              envy\n",
       "14990             worry\n",
       "15017              deny\n",
       "15653          flip_out\n",
       "15695           mortify\n",
       "16321              weep\n",
       "16327    piece_together\n",
       "16339               say\n",
       "16352           terrify\n",
       "16372          come_out\n",
       "17009         freak_out\n",
       "17013              pity\n",
       "17025             write\n",
       "17050         point_out\n",
       "17058           horrify\n",
       "17690           specify\n",
       "17704        reason_out\n",
       "17706          flip_out\n",
       "17714          identify\n",
       "17721           horrify\n",
       "17727           clarify\n",
       "18377            notify\n",
       "18379         spellbind\n",
       "18387            verify\n",
       "18412              know\n",
       "19049               see\n",
       "19053             reply\n",
       "19066          find_out\n",
       "19082             swear\n",
       "19085              tell\n",
       "19087        figure_out\n",
       "19089         freak_out\n",
       "19102           stupefy\n",
       "19727    piece_together\n",
       "19729          foretell\n",
       "19754           mislead\n",
       "19772          prophesy\n",
       "20436           fess_up\n",
       "20445          come_out\n",
       "20447           satisfy\n",
       "20466           mystify\n",
       "21084           gratify\n",
       "21090           signify\n",
       "21100           testify\n",
       "21116             worry\n",
       "21132          flip_out\n",
       "21139            notify\n",
       "21142        dissatisfy\n",
       "21766           sign_up\n",
       "21776           mortify\n",
       "21778         point_out\n",
       "21788           terrify\n",
       "21803           sign_on\n",
       "21804             swear\n",
       "21810             lobby\n",
       "22336           sign_on\n",
       "22338           fess_up\n",
       "22344          turn_out\n",
       "22352           sign_on\n",
       "22373         start_off\n",
       "22384             think\n",
       "22904          identify\n",
       "22916           sign_up\n",
       "22952              bear\n",
       "23506            uphold\n",
       "23513              tell\n",
       "23526               cry\n",
       "24046            forget\n",
       "24073             lobby\n",
       "24079              deny\n",
       "24083           qualify\n",
       "24625           gratify\n",
       "24627           certify\n",
       "24631           sign_up\n",
       "24632             bully\n",
       "24634              know\n",
       "24637              deny\n",
       "24641             plead\n",
       "24652             fancy\n",
       "25196               get\n",
       "25201              find\n",
       "25209            forbid\n",
       "25212           sign_on\n",
       "25231             fight\n",
       "25235             fancy\n",
       "25750            verify\n",
       "25753         point_out\n",
       "25757           sign_up\n",
       "25761               try\n",
       "25769         freak_out\n",
       "25772          come_out\n",
       "26325           gratify\n",
       "26334           specify\n",
       "26355            forget\n",
       "26364          come_out\n",
       "26900           petrify\n",
       "26917             swear\n",
       "26936           clarify\n",
       "26937            choose\n",
       "26940              bear\n",
       "27464                be\n",
       "27476             apply\n",
       "27483            verify\n",
       "27515             fancy\n",
       "28050          foretell\n",
       "28078          make_out\n",
       "28608             imply\n",
       "28611            choose\n",
       "28638              seek\n",
       "28645           mislead\n",
       "28651          identify\n",
       "29184          make_out\n",
       "29219           sign_up\n",
       "29749           signify\n",
       "29756           horrify\n",
       "29757           mislead\n",
       "29761              know\n",
       "30310           satisfy\n",
       "30332             write\n",
       "30348           signify\n",
       "30349              tell\n",
       "30356             fight\n",
       "30365             swear\n",
       "30886              envy\n",
       "30912         undertake\n",
       "30914         set_about\n",
       "30932              find\n",
       "31457              lead\n",
       "31472            uphold\n",
       "31486           certify\n",
       "32029            choose\n",
       "32047           qualify\n",
       "32050          identify\n",
       "32051           sign_up\n",
       "32057             curse\n",
       "32592            choose\n",
       "32595       come_around\n",
       "32596           gratify\n",
       "32611         set_about\n",
       "32615              make\n",
       "32638              tell\n",
       "32640             study\n",
       "33167            choose\n",
       "33174              envy\n",
       "33180               get\n",
       "33187            forget\n",
       "33207        figure_out\n",
       "33733           terrify\n",
       "33746            uphold\n",
       "33776           petrify\n",
       "33785              make\n",
       "34301              lead\n",
       "34312              send\n",
       "34331              tell\n",
       "34343           gratify\n",
       "34352             plead\n",
       "34354           set_out\n",
       "34899        understand\n",
       "34901           satisfy\n",
       "34918           terrify\n",
       "34924           clarify\n",
       "35440         set_about\n",
       "35449              find\n",
       "35450        understand\n",
       "35452        figure_out\n",
       "35476           testify\n",
       "36017             lobby\n",
       "36026         set_about\n",
       "36032          flip_out\n",
       "36035          find_out\n",
       "36036              seek\n",
       "36041               get\n",
       "36586          turn_out\n",
       "36592            forbid\n",
       "36597           certify\n",
       "36598          make_out\n",
       "36606          turn_out\n",
       "36608        understand\n",
       "37159           specify\n",
       "37197           set_out\n",
       "37199           mortify\n",
       "37200           mortify\n",
       "37729             bully\n",
       "37735          identify\n",
       "37737             fight\n",
       "37739             fancy\n",
       "38291           certify\n",
       "38293             teach\n",
       "38304          foretell\n",
       "38309              seek\n",
       "38330           sign_up\n",
       "38344            choose\n",
       "38862             teach\n",
       "38870              make\n",
       "38878               cry\n",
       "38894          flip_out\n",
       "38896           qualify\n",
       "38901           mystify\n",
       "39433           terrify\n",
       "39437              send\n",
       "39442            forbid\n",
       "39448             teach\n",
       "39453           petrify\n",
       "39455           petrify\n",
       "39458           horrify\n",
       "39481             curse\n",
       "40005               try\n",
       "40010           glorify\n",
       "40026             begin\n",
       "40027              send\n",
       "40029              tell\n",
       "40030         set_about\n",
       "40033           mislead\n",
       "40036         set_about\n",
       "40046             teach\n",
       "40572          prophesy\n",
       "40578        understand\n",
       "40583             imply\n",
       "40600       come_around\n",
       "40623           set_out\n",
       "41142              tell\n",
       "41145        dissatisfy\n",
       "41146           shut_up\n",
       "41150              weep\n",
       "41152              meet\n",
       "41167           shut_up\n",
       "41188          prophesy\n",
       "41721           sign_on\n",
       "41723             think\n",
       "41742             lobby\n",
       "41754             imply\n",
       "41762             fight\n",
       "41764           qualify\n",
       "42285              know\n",
       "42294              deny\n",
       "42300               say\n",
       "42307             stand\n",
       "42320            choose\n",
       "42324         start_off\n",
       "42325           sign_on\n",
       "42333               say\n",
       "42851             think\n",
       "42861               buy\n",
       "42889           specify\n",
       "42891              make\n",
       "42896             fancy\n",
       "42898           set_out\n",
       "43424             fancy\n",
       "43435            notify\n",
       "43440             lobby\n",
       "43441            forget\n",
       "43444             teach\n",
       "43460           sign_on\n",
       "43466             fancy\n",
       "43471           horrify\n",
       "43991              send\n",
       "43996          find_out\n",
       "44003           sign_on\n",
       "44012          prophesy\n",
       "44015             bully\n",
       "44030           certify\n",
       "44031               say\n",
       "44045           glorify\n",
       "44560              seek\n",
       "44562             apply\n",
       "44565           mystify\n",
       "44579           sign_up\n",
       "44586             bully\n",
       "44603          make_out\n",
       "44610               try\n",
       "45134           fess_up\n",
       "45148           sign_on\n",
       "45156           sign_up\n",
       "45161            forbid\n",
       "45167             swear\n",
       "45183              meet\n",
       "45720           sign_up\n",
       "45723               buy\n",
       "45740           testify\n",
       "46282           shut_up\n",
       "46290           shut_up\n",
       "46292             begin\n",
       "46295       come_around\n",
       "46305        understand\n",
       "46306              seek\n",
       "46310              make\n",
       "46326               say\n",
       "46840           set_out\n",
       "46850          turn_out\n",
       "46859         freak_out\n",
       "46877         point_out\n",
       "46890           sign_up\n",
       "47418             fancy\n",
       "47441              seek\n",
       "47446            choose\n",
       "47449         point_out\n",
       "47988              come\n",
       "48018             swear\n",
       "48021             imply\n",
       "48560           horrify\n",
       "48561        dissatisfy\n",
       "48577              come\n",
       "48579       come_around\n",
       "48582           qualify\n",
       "48587              take\n",
       "48588             teach\n",
       "48592             swear\n",
       "48595              lead\n",
       "48605           set_out\n",
       "49120           sign_on\n",
       "49134                be\n",
       "49149             curse\n",
       "49703             lobby\n",
       "49712           mortify\n",
       "49718           sign_up\n",
       "49726               say\n",
       "49737           foresee\n",
       "49739                go\n",
       "26             facinate\n",
       "49             facinate\n",
       "Name: Verb, dtype: object"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty[\"Verb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for n in range(0,len(empty)):\n",
    "    m = [[i,j] for i,j in zip(empty[\"VerbListLemmatized\"].iloc[n],empty[\"VerbList\"].iloc[n])]\n",
    "    l.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_list = [i for i in empty[\"Verb\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the verb that's misspelled 'facinate'..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['say',\n",
       " 'say',\n",
       " 'tell',\n",
       " 'think',\n",
       " 'think',\n",
       " 'think',\n",
       " 'think',\n",
       " 'think',\n",
       " 'think',\n",
       " 'think',\n",
       " 'think',\n",
       " 'think',\n",
       " 'think',\n",
       " 'think',\n",
       " 'think',\n",
       " 'think',\n",
       " 'think',\n",
       " 'think',\n",
       " 'think',\n",
       " 'know',\n",
       " 'think',\n",
       " 'think',\n",
       " 'think',\n",
       " 'think',\n",
       " 'feel',\n",
       " 'know',\n",
       " 'forget',\n",
       " 'forget',\n",
       " 'forget',\n",
       " 'forget',\n",
       " 'know',\n",
       " 'know',\n",
       " 'know',\n",
       " 'realize',\n",
       " 'realize',\n",
       " 'realize',\n",
       " 'realize',\n",
       " 'know',\n",
       " 'say',\n",
       " 'say',\n",
       " 'know',\n",
       " 'tell',\n",
       " 'tell',\n",
       " 'think',\n",
       " 'think',\n",
       " 'think',\n",
       " 'think',\n",
       " 'think',\n",
       " 'think',\n",
       " 'think',\n",
       " 'forget',\n",
       " 'forget',\n",
       " 'know',\n",
       " 'know',\n",
       " 'know',\n",
       " 'know',\n",
       " 'realize',\n",
       " 'say',\n",
       " 'say',\n",
       " 'say',\n",
       " 'say',\n",
       " 'tell',\n",
       " 'think',\n",
       " 'say',\n",
       " 'say',\n",
       " 'say',\n",
       " 'say',\n",
       " 'say',\n",
       " 'say',\n",
       " 'say',\n",
       " 'say',\n",
       " 'tell',\n",
       " 'tell',\n",
       " 'tell',\n",
       " 'tell',\n",
       " 'tell',\n",
       " 'tell',\n",
       " 'tell',\n",
       " 'tell',\n",
       " 'think',\n",
       " 'think',\n",
       " 'think',\n",
       " 'think',\n",
       " 'think',\n",
       " 'think',\n",
       " 'think',\n",
       " 'think',\n",
       " 'think',\n",
       " 'think',\n",
       " 'think',\n",
       " 'feel',\n",
       " 'feel',\n",
       " 'feel',\n",
       " 'feel',\n",
       " 'feel',\n",
       " 'find',\n",
       " 'forget',\n",
       " 'forget',\n",
       " 'hope',\n",
       " 'know',\n",
       " 'know',\n",
       " 'know',\n",
       " 'find',\n",
       " 'find',\n",
       " 'find',\n",
       " 'realize',\n",
       " 'say',\n",
       " 'say',\n",
       " 'say',\n",
       " 'say',\n",
       " 'say',\n",
       " 'think',\n",
       " 'think',\n",
       " 'think',\n",
       " 'think',\n",
       " 'say',\n",
       " 'see',\n",
       " 'foresee',\n",
       " 'know',\n",
       " 'think',\n",
       " 'think',\n",
       " 'hope',\n",
       " 'say',\n",
       " 'forget',\n",
       " 'feel',\n",
       " 'deny',\n",
       " 'find_out',\n",
       " 'flip_out',\n",
       " 'forgive',\n",
       " 'terrify',\n",
       " 'clarify',\n",
       " 'weep',\n",
       " 'come_out',\n",
       " 'gratify',\n",
       " 'prophesy',\n",
       " 'worry',\n",
       " 'satisfy',\n",
       " 'mystify',\n",
       " 'figure_out',\n",
       " 'dissatisfy',\n",
       " 'foretell',\n",
       " 'horrify',\n",
       " 'envy',\n",
       " 'curse',\n",
       " 'flip_out',\n",
       " 'say',\n",
       " 'hush_up',\n",
       " 'cry',\n",
       " 'certify',\n",
       " 'testify',\n",
       " 'reason_out',\n",
       " 'piece_together',\n",
       " 'hush_up',\n",
       " 'fess_up',\n",
       " 'freak_out',\n",
       " 'tell',\n",
       " 'foretell',\n",
       " 'horrify',\n",
       " 'freak_out',\n",
       " 'spellbind',\n",
       " 'flip_out',\n",
       " 'come_out',\n",
       " 'satisfy',\n",
       " 'figure_out',\n",
       " 'gratify',\n",
       " 'specify',\n",
       " 'find_out',\n",
       " 'stupefy',\n",
       " 'notify',\n",
       " 'swear',\n",
       " 'fess_up',\n",
       " 'foretell',\n",
       " 'freak_out',\n",
       " 'imply',\n",
       " 'petrify',\n",
       " 'mislead',\n",
       " 'uphold',\n",
       " 'freak_out',\n",
       " 'reason_out',\n",
       " 'point_out',\n",
       " 'feel',\n",
       " 'signify',\n",
       " 'notify',\n",
       " 'worry',\n",
       " 'petrify',\n",
       " 'stupefy',\n",
       " 'mislead',\n",
       " 'spellbind',\n",
       " 'foresee',\n",
       " 'mortify',\n",
       " 'piece_together',\n",
       " 'mortify',\n",
       " 'tell',\n",
       " 'pity',\n",
       " 'think',\n",
       " 'reply',\n",
       " 'mystify',\n",
       " 'point_out',\n",
       " 'write',\n",
       " 'flip_out',\n",
       " 'terrify',\n",
       " 'worry',\n",
       " 'see',\n",
       " 'know',\n",
       " 'identify',\n",
       " 'dissatisfy',\n",
       " 'sing',\n",
       " 'verify',\n",
       " 'understand',\n",
       " 'forget',\n",
       " 'find',\n",
       " 'sing',\n",
       " 'hush_up',\n",
       " 'dissatisfy',\n",
       " 'figure_out',\n",
       " 'petrify',\n",
       " 'worry',\n",
       " 'mislead',\n",
       " 'satisfy',\n",
       " 'stupefy',\n",
       " 'point_out',\n",
       " 'uphold',\n",
       " 'certify',\n",
       " 'hush_up',\n",
       " 'spellbind',\n",
       " 'freak_out',\n",
       " 'gratify',\n",
       " 'imply',\n",
       " 'tell',\n",
       " 'foretell',\n",
       " 'mystify',\n",
       " 'find',\n",
       " 'understand',\n",
       " 'petrify',\n",
       " 'feel',\n",
       " 'freak_out',\n",
       " 'mortify',\n",
       " 'flip_out',\n",
       " 'find_out',\n",
       " 'terrify',\n",
       " 'think',\n",
       " 'fess_up',\n",
       " 'foretell',\n",
       " 'cry',\n",
       " 'curse',\n",
       " 'reason_out',\n",
       " 'forget',\n",
       " 'forgive',\n",
       " 'foresee',\n",
       " 'envy',\n",
       " 'worry',\n",
       " 'deny',\n",
       " 'flip_out',\n",
       " 'mortify',\n",
       " 'weep',\n",
       " 'piece_together',\n",
       " 'say',\n",
       " 'terrify',\n",
       " 'come_out',\n",
       " 'freak_out',\n",
       " 'pity',\n",
       " 'write',\n",
       " 'point_out',\n",
       " 'horrify',\n",
       " 'specify',\n",
       " 'reason_out',\n",
       " 'flip_out',\n",
       " 'identify',\n",
       " 'horrify',\n",
       " 'clarify',\n",
       " 'notify',\n",
       " 'spellbind',\n",
       " 'verify',\n",
       " 'know',\n",
       " 'see',\n",
       " 'reply',\n",
       " 'find_out',\n",
       " 'swear',\n",
       " 'tell',\n",
       " 'figure_out',\n",
       " 'freak_out',\n",
       " 'stupefy',\n",
       " 'piece_together',\n",
       " 'foretell',\n",
       " 'mislead',\n",
       " 'prophesy',\n",
       " 'fess_up',\n",
       " 'come_out',\n",
       " 'satisfy',\n",
       " 'mystify',\n",
       " 'gratify',\n",
       " 'signify',\n",
       " 'testify',\n",
       " 'worry',\n",
       " 'flip_out',\n",
       " 'notify',\n",
       " 'dissatisfy',\n",
       " 'sign_up',\n",
       " 'mortify',\n",
       " 'point_out',\n",
       " 'terrify',\n",
       " 'sign_on',\n",
       " 'swear',\n",
       " 'lobby',\n",
       " 'sign_on',\n",
       " 'fess_up',\n",
       " 'turn_out',\n",
       " 'sign_on',\n",
       " 'start_off',\n",
       " 'think',\n",
       " 'identify',\n",
       " 'sign_up',\n",
       " 'bear',\n",
       " 'uphold',\n",
       " 'tell',\n",
       " 'cry',\n",
       " 'forget',\n",
       " 'lobby',\n",
       " 'deny',\n",
       " 'qualify',\n",
       " 'gratify',\n",
       " 'certify',\n",
       " 'sign_up',\n",
       " 'bully',\n",
       " 'know',\n",
       " 'deny',\n",
       " 'plead',\n",
       " 'fancy',\n",
       " 'get',\n",
       " 'find',\n",
       " 'forbid',\n",
       " 'sign_on',\n",
       " 'fight',\n",
       " 'fancy',\n",
       " 'verify',\n",
       " 'point_out',\n",
       " 'sign_up',\n",
       " 'try',\n",
       " 'freak_out',\n",
       " 'come_out',\n",
       " 'gratify',\n",
       " 'specify',\n",
       " 'forget',\n",
       " 'come_out',\n",
       " 'petrify',\n",
       " 'swear',\n",
       " 'clarify',\n",
       " 'choose',\n",
       " 'bear',\n",
       " 'be',\n",
       " 'apply',\n",
       " 'verify',\n",
       " 'fancy',\n",
       " 'foretell',\n",
       " 'make_out',\n",
       " 'imply',\n",
       " 'choose',\n",
       " 'seek',\n",
       " 'mislead',\n",
       " 'identify',\n",
       " 'make_out',\n",
       " 'sign_up',\n",
       " 'signify',\n",
       " 'horrify',\n",
       " 'mislead',\n",
       " 'know',\n",
       " 'satisfy',\n",
       " 'write',\n",
       " 'signify',\n",
       " 'tell',\n",
       " 'fight',\n",
       " 'swear',\n",
       " 'envy',\n",
       " 'undertake',\n",
       " 'set_about',\n",
       " 'find',\n",
       " 'lead',\n",
       " 'uphold',\n",
       " 'certify',\n",
       " 'choose',\n",
       " 'qualify',\n",
       " 'identify',\n",
       " 'sign_up',\n",
       " 'curse',\n",
       " 'choose',\n",
       " 'come_around',\n",
       " 'gratify',\n",
       " 'set_about',\n",
       " 'make',\n",
       " 'tell',\n",
       " 'study',\n",
       " 'choose',\n",
       " 'envy',\n",
       " 'get',\n",
       " 'forget',\n",
       " 'figure_out',\n",
       " 'terrify',\n",
       " 'uphold',\n",
       " 'petrify',\n",
       " 'make',\n",
       " 'lead',\n",
       " 'send',\n",
       " 'tell',\n",
       " 'gratify',\n",
       " 'plead',\n",
       " 'set_out',\n",
       " 'understand',\n",
       " 'satisfy',\n",
       " 'terrify',\n",
       " 'clarify',\n",
       " 'set_about',\n",
       " 'find',\n",
       " 'understand',\n",
       " 'figure_out',\n",
       " 'testify',\n",
       " 'lobby',\n",
       " 'set_about',\n",
       " 'flip_out',\n",
       " 'find_out',\n",
       " 'seek',\n",
       " 'get',\n",
       " 'turn_out',\n",
       " 'forbid',\n",
       " 'certify',\n",
       " 'make_out',\n",
       " 'turn_out',\n",
       " 'understand',\n",
       " 'specify',\n",
       " 'set_out',\n",
       " 'mortify',\n",
       " 'mortify',\n",
       " 'bully',\n",
       " 'identify',\n",
       " 'fight',\n",
       " 'fancy',\n",
       " 'certify',\n",
       " 'teach',\n",
       " 'foretell',\n",
       " 'seek',\n",
       " 'sign_up',\n",
       " 'choose',\n",
       " 'teach',\n",
       " 'make',\n",
       " 'cry',\n",
       " 'flip_out',\n",
       " 'qualify',\n",
       " 'mystify',\n",
       " 'terrify',\n",
       " 'send',\n",
       " 'forbid',\n",
       " 'teach',\n",
       " 'petrify',\n",
       " 'petrify',\n",
       " 'horrify',\n",
       " 'curse',\n",
       " 'try',\n",
       " 'glorify',\n",
       " 'begin',\n",
       " 'send',\n",
       " 'tell',\n",
       " 'set_about',\n",
       " 'mislead',\n",
       " 'set_about',\n",
       " 'teach',\n",
       " 'prophesy',\n",
       " 'understand',\n",
       " 'imply',\n",
       " 'come_around',\n",
       " 'set_out',\n",
       " 'tell',\n",
       " 'dissatisfy',\n",
       " 'shut_up',\n",
       " 'weep',\n",
       " 'meet',\n",
       " 'shut_up',\n",
       " 'prophesy',\n",
       " 'sign_on',\n",
       " 'think',\n",
       " 'lobby',\n",
       " 'imply',\n",
       " 'fight',\n",
       " 'qualify',\n",
       " 'know',\n",
       " 'deny',\n",
       " 'say',\n",
       " 'stand',\n",
       " 'choose',\n",
       " 'start_off',\n",
       " 'sign_on',\n",
       " 'say',\n",
       " 'think',\n",
       " 'buy',\n",
       " 'specify',\n",
       " 'make',\n",
       " 'fancy',\n",
       " 'set_out',\n",
       " 'fancy',\n",
       " 'notify',\n",
       " 'lobby',\n",
       " 'forget',\n",
       " 'teach',\n",
       " 'sign_on',\n",
       " 'fancy',\n",
       " 'horrify',\n",
       " 'send',\n",
       " 'find_out',\n",
       " 'sign_on',\n",
       " 'prophesy',\n",
       " 'bully',\n",
       " 'certify',\n",
       " 'say',\n",
       " 'glorify',\n",
       " 'seek',\n",
       " 'apply',\n",
       " 'mystify',\n",
       " 'sign_up',\n",
       " 'bully',\n",
       " 'make_out',\n",
       " 'try',\n",
       " 'fess_up',\n",
       " 'sign_on',\n",
       " 'sign_up',\n",
       " 'forbid',\n",
       " 'swear',\n",
       " 'meet',\n",
       " 'sign_up',\n",
       " 'buy',\n",
       " 'testify',\n",
       " 'shut_up',\n",
       " 'shut_up',\n",
       " 'begin',\n",
       " 'come_around',\n",
       " 'understand',\n",
       " 'seek',\n",
       " 'make',\n",
       " 'say',\n",
       " 'set_out',\n",
       " 'turn_out',\n",
       " 'freak_out',\n",
       " 'point_out',\n",
       " 'sign_up',\n",
       " 'fancy',\n",
       " 'seek',\n",
       " 'choose',\n",
       " 'point_out',\n",
       " 'come',\n",
       " 'swear',\n",
       " 'imply',\n",
       " 'horrify',\n",
       " 'dissatisfy',\n",
       " 'come',\n",
       " 'come_around',\n",
       " 'qualify',\n",
       " 'take',\n",
       " 'teach',\n",
       " 'swear',\n",
       " 'lead',\n",
       " 'set_out',\n",
       " 'sign_on',\n",
       " 'be',\n",
       " 'curse',\n",
       " 'lobby',\n",
       " 'mortify',\n",
       " 'sign_up',\n",
       " 'say',\n",
       " 'foresee',\n",
       " 'go',\n",
       " 'facinate',\n",
       " 'facinate']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for v in empty[\"VerbList\"]:\n",
    "    if v in l[]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "569"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NN'"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty[\"Grouped\"].values[2][1][i][1]\n",
    "empty[\"VerbListLemmatizedTagged\"].values[2][1][i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty[\"Grouped2\"] = empty.Grouped[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "569"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for x, y in zip(xs, ys):\n",
    "    print x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search_two_cols(col1,col2)\n",
    "    for x,y in zip(col1,col2):\n",
    "        l = []\n",
    "\n",
    "        if col1 is in col2.tolist():\n",
    "            l.append(col1)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Verb</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>VerbToken</th>\n",
       "      <th>VerbList</th>\n",
       "      <th>VerbTagged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BNC-1002</td>\n",
       "      <td>say</td>\n",
       "      <td>Indeed it could be said that they had prospered.</td>\n",
       "      <td>None</td>\n",
       "      <td>[(be, VB), (said, VBD), (had, VBD), (prospered, VBN)]</td>\n",
       "      <td>[(say, VB)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BNC-1003</td>\n",
       "      <td>say</td>\n",
       "      <td>He might have said to her that some time in the middle of the nineteenth century a cult had grown up around the idea of the home.</td>\n",
       "      <td>None</td>\n",
       "      <td>[(have, VB), (said, VBD), (had, VBD), (grown, VBN)]</td>\n",
       "      <td>[(say, VB)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>BNC-1145</td>\n",
       "      <td>tell</td>\n",
       "      <td>She could also have told this was Tina's mother before Mrs Darne went off down the passage that led to the Headmaster's Flat.</td>\n",
       "      <td>None</td>\n",
       "      <td>[(have, VB), (told, VBN), (was, VBD), (went, VBD), (led, VBD)]</td>\n",
       "      <td>[(tell, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>BNC-1187</td>\n",
       "      <td>think</td>\n",
       "      <td>They may have thought they were putting it out of its misery - a lifetime beautifying the lorry-route to the A1.</td>\n",
       "      <td>None</td>\n",
       "      <td>[(have, VB), (thought, VBN), (were, VBD), (putting, VBG), (beautifying, VBG)]</td>\n",
       "      <td>[(think, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>BNC-1194</td>\n",
       "      <td>think</td>\n",
       "      <td>Perhaps he thought that her own wishes would hardly be considered in the matter.</td>\n",
       "      <td>None</td>\n",
       "      <td>[(thought, VBD), (be, VB), (considered, VBN)]</td>\n",
       "      <td>[(think, NN)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID   Verb  \\\n",
       "9    BNC-1002    say   \n",
       "17   BNC-1003    say   \n",
       "575  BNC-1145   tell   \n",
       "716  BNC-1187  think   \n",
       "733  BNC-1194  think   \n",
       "\n",
       "                                                                                                                              Sentence  \\\n",
       "9                                                                                     Indeed it could be said that they had prospered.   \n",
       "17   He might have said to her that some time in the middle of the nineteenth century a cult had grown up around the idea of the home.   \n",
       "575      She could also have told this was Tina's mother before Mrs Darne went off down the passage that led to the Headmaster's Flat.   \n",
       "716                   They may have thought they were putting it out of its misery - a lifetime beautifying the lorry-route to the A1.   \n",
       "733                                                   Perhaps he thought that her own wishes would hardly be considered in the matter.   \n",
       "\n",
       "    VerbToken  \\\n",
       "9        None   \n",
       "17       None   \n",
       "575      None   \n",
       "716      None   \n",
       "733      None   \n",
       "\n",
       "                                                                          VerbList  \\\n",
       "9                            [(be, VB), (said, VBD), (had, VBD), (prospered, VBN)]   \n",
       "17                             [(have, VB), (said, VBD), (had, VBD), (grown, VBN)]   \n",
       "575                 [(have, VB), (told, VBN), (was, VBD), (went, VBD), (led, VBD)]   \n",
       "716  [(have, VB), (thought, VBN), (were, VBD), (putting, VBG), (beautifying, VBG)]   \n",
       "733                                  [(thought, VBD), (be, VB), (considered, VBN)]   \n",
       "\n",
       "        VerbTagged  \n",
       "9      [(say, VB)]  \n",
       "17     [(say, VB)]  \n",
       "575   [(tell, NN)]  \n",
       "716  [(think, NN)]  \n",
       "733  [(think, NN)]  "
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the dataframes together again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask out the VerbToken from Sentence\n",
    "\n",
    "- Once the issues above are worked out, then the rest of this should be pretty straightforward.\n",
    "\n",
    "- For discussion about which model is best, check out the following twitter thread: https://twitter.com/bruno_nicenboim/status/1379168059311656963\n",
    "\n",
    "- Probably we should use GPT3, not BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Masked\"] = df.apply(lambda x: x['Sentence'].replace(x[\"VerbToken\"],\"[MASK]\"),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked language modeling to estimate surprisal\n",
    "\n",
    "- Info on fill-mask pipeline: https://huggingface.co/transformers/main_classes/pipelines.html#transformers.FillMaskPipeline\n",
    "- Info on particular models: https://huggingface.co/models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "unmasker = pipeline('fill-mask', model='bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unmasker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-9ee828699290>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# using indexing and key to get the relevant output score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0munmasker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dana was [MASK] that Mars has no water.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"surprised\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'unmasker' is not defined"
     ]
    }
   ],
   "source": [
    "# using indexing and key to get the relevant output score\n",
    "unmasker(\"Dana was [MASK] that Mars has no water.\",targets=\"surprised\")[0]['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlm_over_df(input_df):\n",
    "    for row in input_df.itterows():\n",
    "        sentence = f\"{s}\".format(s=input_df[\"sentence\"])\n",
    "        verb = f\"{v}\".format(v=input_df[\"verb\"])\n",
    "        mask_fill = unmasker(sentence, targets=verb)\n",
    "        input_df[\"mlm_score\"] = mask_fill[0]['score']\n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Sentence_Masked\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating surprisal from language models\n",
    "Take sentences from CommitmentBank, MegaAttitudes, and stimuli from experiment, mask the attitude predicate, and get predicted probability of occurrence for the target verb. Then, calculate from that the surprisal of the verb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This makes the display show more info\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "1. [Read in the three datasets](#Read-in-the-three-datasets)\n",
    "2. [Masking out the correct verb](#Masking-out-the-correct-verb)\n",
    "    1. [Remaining cases](#Remaining-cases)\n",
    "    2. [Proposed Solution](#Proposed-Solution)\n",
    "        1. [Step 1. Create a new column with list of pos tagged verbs from Sentence](#Step-1.-Create-a-new-column-with-list-of-pos-tagged-verbs-from-Sentence)\n",
    "        2. [Step 2. Lemmatize VerbList](#Step-2.-Lemmatize-VerbList)\n",
    "        3. [TROUBLESHOOT NEEDED](#TROUBLESHOOT-NEEDED)\n",
    "    3. [Combine the datafriends together again](#Combine-the-dataframes-together-again)\n",
    "    4. [Mask out the VerbToken from Sentence](#Mask-out-the-VerbToken-from-Sentence)\n",
    "4. [Masked language modeling to estimate surprisal](#Masked-language-modeling-to-estimate-surprisal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the three datasets\n",
    "- Subset the dfs to just the relevant columns: ID, Verb, Sentence\n",
    "- Make sure that the column names are consistent across the tree dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CommitmentBank\n",
    "# raw url: https://raw.githubusercontent.com/khuyen-le/projectivity-factors/master/data/CommitmentBank-All.csv\n",
    "cb = pd.read_csv(\"../data/CommitmentBank-ALL.csv\")[[\"uID\",\"Verb\",\"Target\"]].drop_duplicates()\n",
    "cb = cb.rename(columns={\"Target\": \"Sentence\",\"uID\":\"ID\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MegaVeridicality\n",
    "# raw URL: https://raw.githubusercontent.com/khuyen-le/projectivity-factors/master/data/mega-veridicality-v2.csv\n",
    "mv = pd.read_csv(\"../data/mega-veridicality-v2.csv\")[[\"verb\",\"frame\",\"voice\",\"sentence\"]].drop_duplicates()\n",
    "mv = mv.rename(columns={\"verb\": \"Verb\", \"sentence\":\"Sentence\"})\n",
    "mv[\"ID\"] = mv[['frame', 'voice']].apply(lambda x: '_'.join(x), axis=1)\n",
    "mv = mv.drop(columns=[\"frame\",\"voice\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arousal/Valence Study\n",
    "# raw URL: https://raw.githubusercontent.com/khuyen-le/projectivity-factors/master/data/1_sliderprojection/exp1_test-trials.csv\n",
    "vs = pd.read_csv(\"../data/1_sliderprojection/exp1_test-trials.csv\")[[\"Word\",\"utterance\",\"exp\"]]\n",
    "vs = vs[vs[\"exp\"]==\"stim\"].drop_duplicates().drop(columns={\"exp\"})\n",
    "vs = vs.rename(columns={\"Word\": \"Verb\",\"utterance\":\"Sentence\"})\n",
    "vs[\"ID\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine them together into one df\n",
    "df = pd.concat([cb,mv,vs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Verb</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BNC-1</td>\n",
       "      <td>admit</td>\n",
       "      <td>They were still close enough to shore for him to return her to the police if she admitted she was not an experienced ocean sailor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BNC-1002</td>\n",
       "      <td>say</td>\n",
       "      <td>Indeed it could be said that they had prospered.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BNC-1003</td>\n",
       "      <td>say</td>\n",
       "      <td>He might have said to her that some time in the middle of the nineteenth century a cult had grown up around the idea of the home.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>BNC-1005</td>\n",
       "      <td>say</td>\n",
       "      <td>Of course she could say it was for the children as people always did... It was true up to a point.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>BNC-1006</td>\n",
       "      <td>say</td>\n",
       "      <td>Robyn swallowed and took a deep breath trying to compose herself so that when he returned she could say that it was all right she felt fine now.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID   Verb  \\\n",
       "0      BNC-1  admit   \n",
       "9   BNC-1002    say   \n",
       "17  BNC-1003    say   \n",
       "29  BNC-1005    say   \n",
       "37  BNC-1006    say   \n",
       "\n",
       "                                                                                                                                            Sentence  \n",
       "0                 They were still close enough to shore for him to return her to the police if she admitted she was not an experienced ocean sailor.  \n",
       "9                                                                                                   Indeed it could be said that they had prospered.  \n",
       "17                 He might have said to her that some time in the middle of the nineteenth century a cult had grown up around the idea of the home.  \n",
       "29                                                Of course she could say it was for the children as people always did... It was true up to a point.  \n",
       "37  Robyn swallowed and took a deep breath trying to compose herself so that when he returned she could say that it was all right she felt fine now.  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the correct verb token\n",
    "What we need to do is mask out the correct verb in each of the sentences. We have the correct verb in the Verb column. We can easily use apply() with str.replace() to switch the verb with [MASK]. The problem is that the verbs in the sentences are inflected tokens, while the verbs in Verb are lemmatized.\n",
    "\n",
    "\n",
    "For some of the verbs, we don't need to worry about this problem because there is morphological overlap between the Verb Token and the Verb Lemma. \n",
    "\n",
    "\n",
    "Solution:\n",
    "1. Create a new verb token column\n",
    "2. Regex + literal string interpolation to match works in cases where the Verb matches morphologically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frustratingly, this isn't working\n",
    "# df[\"VerbToken\"] = df['Sentence'].str.extract(fr'({df[\"Verb\"]}\\w*)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find a match in the Sentence column for the verb from the Verb column using a regex re.search() returns a match object, so you have to call .group() to get the string that is matched. In cases where there is no match, a NoneType object is returned and you can't call .group() on that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"VerbToken\"] = df.apply(lambda x: re.search(fr'({x[\"Verb\"]}\\w*)',x['Sentence']), axis=1)\n",
    "\n",
    "# In some cases there is nothing captured, it returns a NoneType and causes the code to fail\n",
    "# because NoneType has no method .group()\n",
    "df[\"VerbToken\"] = df[\"VerbToken\"].apply(lambda x: x.group() if x is not None else x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remaining cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cases where the above solution did not work\n",
    "empty = df[df[\"VerbToken\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.060509554140127"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(empty)/len(df)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed Solution\n",
    "Overarching: lemmatize Sentence, find the verb lemma that matches the respective Verb column. But we actually need the actual verb token not the lemma, because to replace the correct verb in Sentence with [Mask], we will need to extract the relevant token in order to do a successful str.replace().\n",
    "\n",
    "More concrete:\n",
    "1. Make a new column with POS tag verbs from Sentence\n",
    "2. Lemmatize the verbs from the new column\n",
    "3. Here there be dragons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from: https://gaurav5430.medium.com/using-nltk-for-lemmatizing-sentences-c1bfff963258\n",
    "\n",
    "# initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "# def lemmatize_sentence(sentence):\n",
    "#     #tokenize the sentence and find the POS tag for each token\n",
    "#     nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "#     #tuple of (token, wordnet_tag)\n",
    "#     wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "#     lemmatized_sentence = []\n",
    "#     for word, tag in wordnet_tagged:\n",
    "#         if tag is None:\n",
    "#             #if there is no available tag, append the token as is\n",
    "#             lemmatized_sentence.append(word)\n",
    "#         else:        \n",
    "#             #else use the tag to lemmatize the token\n",
    "#             lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "#     return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Failed code attempt to do it all in one\n",
    "\n",
    "```\n",
    "def lemmatize_verb_from_sentence(sentence,verb):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lw = []\n",
    "#     for i in range(0,len(empty)-1):\n",
    "#     for v in empty[\"Verb\"].values:\n",
    "#         verb_from_empty = empty[\"Verb\"].values[i]\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            continue\n",
    "        elif tag != 'v':\n",
    "            continue\n",
    "        else:\n",
    "            lemma = lemmatizer.lemmatize(word, tag)\n",
    "            if lemma != verb:\n",
    "                # Go to the next word/tag pair to find the relevant verb\n",
    "                break\n",
    "            elif lemma == verb:\n",
    "                print(\"{verb}: {word} {lemma}\".format(verb=verb,word=word,lemma=lemma))\n",
    "                lw.append(word)\n",
    "                lw.append(lemma)\n",
    "#     print(lw)\n",
    "    return ' '.join(lw)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Create a new column with list of pos tagged verbs from Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verb(sentence):\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "    verbs = []\n",
    "    for i in nltk_tagged:\n",
    "        if 'VB' in i[1]:\n",
    "            verbs.append(i)\n",
    "    return verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-463aecabe8aa>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  empty[\"VerbList\"] = empty[\"Sentence\"].apply(lambda x: get_verb(x))\n"
     ]
    }
   ],
   "source": [
    "empty[\"VerbList\"] = empty[\"Sentence\"].apply(lambda x: get_verb(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-8b41673c2e32>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  empty[\"VerbTagged\"] = empty[\"Verb\"].apply(lambda x: nltk.pos_tag([x]))\n"
     ]
    }
   ],
   "source": [
    "# this is possibly not necessary\n",
    "# empty[\"VerbTagged\"] = empty[\"Verb\"].apply(lambda x: nltk.pos_tag([x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Lemmatize VerbList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_from_nltk_tagged_list(nltk_tagged):\n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-8a1491a9a5a8>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  empty[\"VerbListLemmatized\"] = empty[\"VerbList\"].apply(lambda x: lemmatize_from_nltk_tagged_list(x))\n"
     ]
    }
   ],
   "source": [
    "empty[\"VerbListLemmatized\"] = empty[\"VerbList\"].apply(lambda x: lemmatize_from_nltk_tagged_list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Verb</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>VerbToken</th>\n",
       "      <th>VerbList</th>\n",
       "      <th>VerbTagged</th>\n",
       "      <th>VerbListLemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BNC-1002</td>\n",
       "      <td>say</td>\n",
       "      <td>Indeed it could be said that they had prospered.</td>\n",
       "      <td>None</td>\n",
       "      <td>[(be, VB), (said, VBD), (had, VBD), (prospered, VBN)]</td>\n",
       "      <td>[(say, VB)]</td>\n",
       "      <td>[be, say, have, prosper]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BNC-1003</td>\n",
       "      <td>say</td>\n",
       "      <td>He might have said to her that some time in the middle of the nineteenth century a cult had grown up around the idea of the home.</td>\n",
       "      <td>None</td>\n",
       "      <td>[(have, VB), (said, VBD), (had, VBD), (grown, VBN)]</td>\n",
       "      <td>[(say, VB)]</td>\n",
       "      <td>[have, say, have, grow]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>BNC-1145</td>\n",
       "      <td>tell</td>\n",
       "      <td>She could also have told this was Tina's mother before Mrs Darne went off down the passage that led to the Headmaster's Flat.</td>\n",
       "      <td>None</td>\n",
       "      <td>[(have, VB), (told, VBN), (was, VBD), (went, VBD), (led, VBD)]</td>\n",
       "      <td>[(tell, NN)]</td>\n",
       "      <td>[have, tell, be, go, lead]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>BNC-1187</td>\n",
       "      <td>think</td>\n",
       "      <td>They may have thought they were putting it out of its misery - a lifetime beautifying the lorry-route to the A1.</td>\n",
       "      <td>None</td>\n",
       "      <td>[(have, VB), (thought, VBN), (were, VBD), (putting, VBG), (beautifying, VBG)]</td>\n",
       "      <td>[(think, NN)]</td>\n",
       "      <td>[have, think, be, put, beautify]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>BNC-1194</td>\n",
       "      <td>think</td>\n",
       "      <td>Perhaps he thought that her own wishes would hardly be considered in the matter.</td>\n",
       "      <td>None</td>\n",
       "      <td>[(thought, VBD), (be, VB), (considered, VBN)]</td>\n",
       "      <td>[(think, NN)]</td>\n",
       "      <td>[think, be, consider]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID   Verb  \\\n",
       "9    BNC-1002    say   \n",
       "17   BNC-1003    say   \n",
       "575  BNC-1145   tell   \n",
       "716  BNC-1187  think   \n",
       "733  BNC-1194  think   \n",
       "\n",
       "                                                                                                                              Sentence  \\\n",
       "9                                                                                     Indeed it could be said that they had prospered.   \n",
       "17   He might have said to her that some time in the middle of the nineteenth century a cult had grown up around the idea of the home.   \n",
       "575      She could also have told this was Tina's mother before Mrs Darne went off down the passage that led to the Headmaster's Flat.   \n",
       "716                   They may have thought they were putting it out of its misery - a lifetime beautifying the lorry-route to the A1.   \n",
       "733                                                   Perhaps he thought that her own wishes would hardly be considered in the matter.   \n",
       "\n",
       "    VerbToken  \\\n",
       "9        None   \n",
       "17       None   \n",
       "575      None   \n",
       "716      None   \n",
       "733      None   \n",
       "\n",
       "                                                                          VerbList  \\\n",
       "9                            [(be, VB), (said, VBD), (had, VBD), (prospered, VBN)]   \n",
       "17                             [(have, VB), (said, VBD), (had, VBD), (grown, VBN)]   \n",
       "575                 [(have, VB), (told, VBN), (was, VBD), (went, VBD), (led, VBD)]   \n",
       "716  [(have, VB), (thought, VBN), (were, VBD), (putting, VBG), (beautifying, VBG)]   \n",
       "733                                  [(thought, VBD), (be, VB), (considered, VBN)]   \n",
       "\n",
       "        VerbTagged                VerbListLemmatized  \n",
       "9      [(say, VB)]          [be, say, have, prosper]  \n",
       "17     [(say, VB)]           [have, say, have, grow]  \n",
       "575   [(tell, NN)]        [have, tell, be, go, lead]  \n",
       "716  [(think, NN)]  [have, think, be, put, beautify]  \n",
       "733  [(think, NN)]             [think, be, consider]  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-714684832097>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  empty[\"VerbListLemmatizedTagged\"] = empty[\"VerbListLemmatized\"].apply(lambda x: nltk.pos_tag(x))\n"
     ]
    }
   ],
   "source": [
    "empty[\"VerbListLemmatizedTagged\"] = empty[\"VerbListLemmatized\"].apply(lambda x: nltk.pos_tag(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Verb</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>VerbToken</th>\n",
       "      <th>VerbList</th>\n",
       "      <th>VerbTagged</th>\n",
       "      <th>VerbListLemmatized</th>\n",
       "      <th>VerbListLemmatizedTagged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BNC-1002</td>\n",
       "      <td>say</td>\n",
       "      <td>Indeed it could be said that they had prospered.</td>\n",
       "      <td>None</td>\n",
       "      <td>[(be, VB), (said, VBD), (had, VBD), (prospered, VBN)]</td>\n",
       "      <td>[(say, VB)]</td>\n",
       "      <td>[be, say, have, prosper]</td>\n",
       "      <td>[(be, VB), (say, VBN), (have, VBP), (prosper, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BNC-1003</td>\n",
       "      <td>say</td>\n",
       "      <td>He might have said to her that some time in the middle of the nineteenth century a cult had grown up around the idea of the home.</td>\n",
       "      <td>None</td>\n",
       "      <td>[(have, VB), (said, VBD), (had, VBD), (grown, VBN)]</td>\n",
       "      <td>[(say, VB)]</td>\n",
       "      <td>[have, say, have, grow]</td>\n",
       "      <td>[(have, VBP), (say, VBN), (have, VBP), (grow, NNS)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>BNC-1145</td>\n",
       "      <td>tell</td>\n",
       "      <td>She could also have told this was Tina's mother before Mrs Darne went off down the passage that led to the Headmaster's Flat.</td>\n",
       "      <td>None</td>\n",
       "      <td>[(have, VB), (told, VBN), (was, VBD), (went, VBD), (led, VBD)]</td>\n",
       "      <td>[(tell, NN)]</td>\n",
       "      <td>[have, tell, be, go, lead]</td>\n",
       "      <td>[(have, VB), (tell, NN), (be, VB), (go, VBN), (lead, JJ)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>BNC-1187</td>\n",
       "      <td>think</td>\n",
       "      <td>They may have thought they were putting it out of its misery - a lifetime beautifying the lorry-route to the A1.</td>\n",
       "      <td>None</td>\n",
       "      <td>[(have, VB), (thought, VBN), (were, VBD), (putting, VBG), (beautifying, VBG)]</td>\n",
       "      <td>[(think, NN)]</td>\n",
       "      <td>[have, think, be, put, beautify]</td>\n",
       "      <td>[(have, VB), (think, NN), (be, VB), (put, VBN), (beautify, VB)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>BNC-1194</td>\n",
       "      <td>think</td>\n",
       "      <td>Perhaps he thought that her own wishes would hardly be considered in the matter.</td>\n",
       "      <td>None</td>\n",
       "      <td>[(thought, VBD), (be, VB), (considered, VBN)]</td>\n",
       "      <td>[(think, NN)]</td>\n",
       "      <td>[think, be, consider]</td>\n",
       "      <td>[(think, NN), (be, VB), (consider, JJR)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID   Verb  \\\n",
       "9    BNC-1002    say   \n",
       "17   BNC-1003    say   \n",
       "575  BNC-1145   tell   \n",
       "716  BNC-1187  think   \n",
       "733  BNC-1194  think   \n",
       "\n",
       "                                                                                                                              Sentence  \\\n",
       "9                                                                                     Indeed it could be said that they had prospered.   \n",
       "17   He might have said to her that some time in the middle of the nineteenth century a cult had grown up around the idea of the home.   \n",
       "575      She could also have told this was Tina's mother before Mrs Darne went off down the passage that led to the Headmaster's Flat.   \n",
       "716                   They may have thought they were putting it out of its misery - a lifetime beautifying the lorry-route to the A1.   \n",
       "733                                                   Perhaps he thought that her own wishes would hardly be considered in the matter.   \n",
       "\n",
       "    VerbToken  \\\n",
       "9        None   \n",
       "17       None   \n",
       "575      None   \n",
       "716      None   \n",
       "733      None   \n",
       "\n",
       "                                                                          VerbList  \\\n",
       "9                            [(be, VB), (said, VBD), (had, VBD), (prospered, VBN)]   \n",
       "17                             [(have, VB), (said, VBD), (had, VBD), (grown, VBN)]   \n",
       "575                 [(have, VB), (told, VBN), (was, VBD), (went, VBD), (led, VBD)]   \n",
       "716  [(have, VB), (thought, VBN), (were, VBD), (putting, VBG), (beautifying, VBG)]   \n",
       "733                                  [(thought, VBD), (be, VB), (considered, VBN)]   \n",
       "\n",
       "        VerbTagged                VerbListLemmatized  \\\n",
       "9      [(say, VB)]          [be, say, have, prosper]   \n",
       "17     [(say, VB)]           [have, say, have, grow]   \n",
       "575   [(tell, NN)]        [have, tell, be, go, lead]   \n",
       "716  [(think, NN)]  [have, think, be, put, beautify]   \n",
       "733  [(think, NN)]             [think, be, consider]   \n",
       "\n",
       "                                            VerbListLemmatizedTagged  \n",
       "9                 [(be, VB), (say, VBN), (have, VBP), (prosper, NN)]  \n",
       "17               [(have, VBP), (say, VBN), (have, VBP), (grow, NNS)]  \n",
       "575        [(have, VB), (tell, NN), (be, VB), (go, VBN), (lead, JJ)]  \n",
       "716  [(have, VB), (think, NN), (be, VB), (put, VBN), (beautify, VB)]  \n",
       "733                         [(think, NN), (be, VB), (consider, JJR)]  "
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TROUBLESHOOT NEEDED\n",
    "what i want to do is: \n",
    "1. pair each element of the lists together so that each token and it's corresponding lemma are together (zip() should be able to do that, i think we would get a list of lists)\n",
    "2. Then it should be easy to search through the list using indexing for a match with the corresponding verb from Verb on the verb lemma. the match should allow us to return the correct Token,Lemma list\n",
    "3. Once we have that list, there are several solutions, to just index into it to get the VerbToken that matches the verb lemma in the Verb column\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [[x,y] for x,y in zip(list(empty[\"VerbList\"]),list(empty[\"VerbListLemmatizedTagged\"]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('be', 'VB'), ('say', 'VBN'), ('have', 'VBP'), ('prosper', 'NN')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('be', 'VB'), ('said', 'VBD'), ('had', 'VBD'), ('prospered', 'VBN')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NN'"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty[\"Grouped\"].values[2][1][i][1]\n",
    "empty[\"VerbListLemmatizedTagged\"].values[2][1][i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty[\"Grouped2\"] = empty.Grouped[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "569"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for x, y in zip(xs, ys):\n",
    "    print x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search_two_cols(col1,col2)\n",
    "    for x,y in zip(col1,col2):\n",
    "        l = []\n",
    "\n",
    "        if col1 is in col2.tolist():\n",
    "            l.append(col1)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Verb</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>VerbToken</th>\n",
       "      <th>VerbList</th>\n",
       "      <th>VerbTagged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BNC-1002</td>\n",
       "      <td>say</td>\n",
       "      <td>Indeed it could be said that they had prospered.</td>\n",
       "      <td>None</td>\n",
       "      <td>[(be, VB), (said, VBD), (had, VBD), (prospered, VBN)]</td>\n",
       "      <td>[(say, VB)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BNC-1003</td>\n",
       "      <td>say</td>\n",
       "      <td>He might have said to her that some time in the middle of the nineteenth century a cult had grown up around the idea of the home.</td>\n",
       "      <td>None</td>\n",
       "      <td>[(have, VB), (said, VBD), (had, VBD), (grown, VBN)]</td>\n",
       "      <td>[(say, VB)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>BNC-1145</td>\n",
       "      <td>tell</td>\n",
       "      <td>She could also have told this was Tina's mother before Mrs Darne went off down the passage that led to the Headmaster's Flat.</td>\n",
       "      <td>None</td>\n",
       "      <td>[(have, VB), (told, VBN), (was, VBD), (went, VBD), (led, VBD)]</td>\n",
       "      <td>[(tell, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>BNC-1187</td>\n",
       "      <td>think</td>\n",
       "      <td>They may have thought they were putting it out of its misery - a lifetime beautifying the lorry-route to the A1.</td>\n",
       "      <td>None</td>\n",
       "      <td>[(have, VB), (thought, VBN), (were, VBD), (putting, VBG), (beautifying, VBG)]</td>\n",
       "      <td>[(think, NN)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>BNC-1194</td>\n",
       "      <td>think</td>\n",
       "      <td>Perhaps he thought that her own wishes would hardly be considered in the matter.</td>\n",
       "      <td>None</td>\n",
       "      <td>[(thought, VBD), (be, VB), (considered, VBN)]</td>\n",
       "      <td>[(think, NN)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID   Verb  \\\n",
       "9    BNC-1002    say   \n",
       "17   BNC-1003    say   \n",
       "575  BNC-1145   tell   \n",
       "716  BNC-1187  think   \n",
       "733  BNC-1194  think   \n",
       "\n",
       "                                                                                                                              Sentence  \\\n",
       "9                                                                                     Indeed it could be said that they had prospered.   \n",
       "17   He might have said to her that some time in the middle of the nineteenth century a cult had grown up around the idea of the home.   \n",
       "575      She could also have told this was Tina's mother before Mrs Darne went off down the passage that led to the Headmaster's Flat.   \n",
       "716                   They may have thought they were putting it out of its misery - a lifetime beautifying the lorry-route to the A1.   \n",
       "733                                                   Perhaps he thought that her own wishes would hardly be considered in the matter.   \n",
       "\n",
       "    VerbToken  \\\n",
       "9        None   \n",
       "17       None   \n",
       "575      None   \n",
       "716      None   \n",
       "733      None   \n",
       "\n",
       "                                                                          VerbList  \\\n",
       "9                            [(be, VB), (said, VBD), (had, VBD), (prospered, VBN)]   \n",
       "17                             [(have, VB), (said, VBD), (had, VBD), (grown, VBN)]   \n",
       "575                 [(have, VB), (told, VBN), (was, VBD), (went, VBD), (led, VBD)]   \n",
       "716  [(have, VB), (thought, VBN), (were, VBD), (putting, VBG), (beautifying, VBG)]   \n",
       "733                                  [(thought, VBD), (be, VB), (considered, VBN)]   \n",
       "\n",
       "        VerbTagged  \n",
       "9      [(say, VB)]  \n",
       "17     [(say, VB)]  \n",
       "575   [(tell, NN)]  \n",
       "716  [(think, NN)]  \n",
       "733  [(think, NN)]  "
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_verb_from_list(sentence,verb):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lw = []\n",
    "#     for i in range(0,len(empty)-1):\n",
    "#     for v in empty[\"Verb\"].values:\n",
    "#         verb_from_empty = empty[\"Verb\"].values[i]\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            continue\n",
    "        elif tag != 'v':\n",
    "            continue\n",
    "        else:\n",
    "            lemma = lemmatizer.lemmatize(word, tag)\n",
    "            if lemma != verb:\n",
    "                # Go to the next word/tag pair to find the relevant verb\n",
    "                break\n",
    "            elif lemma == verb:\n",
    "                print(\"{verb}: {word} {lemma}\".format(verb=verb,word=word,lemma=lemma))\n",
    "                lw.append(word)\n",
    "                lw.append(lemma)\n",
    "#     print(lw)\n",
    "    return ' '.join(lw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the dataframes together again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask out the VerbToken from Sentence\n",
    "\n",
    "- Once the issues above are worked out, then the rest of this should be pretty straightforward.\n",
    "\n",
    "- For discussion about which model is best, check out the following twitter thread: https://twitter.com/bruno_nicenboim/status/1379168059311656963\n",
    "\n",
    "- Probably we should use GPT3, not BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Masked\"] = df.apply(lambda x: x['Sentence'].replace(x[\"VerbToken\"],\"[MASK]\"),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked language modeling to estimate surprisal\n",
    "\n",
    "- Info on fill-mask pipeline: https://huggingface.co/transformers/main_classes/pipelines.html#transformers.FillMaskPipeline\n",
    "- Info on particular models: https://huggingface.co/models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "unmasker = pipeline('fill-mask', model='bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unmasker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-9ee828699290>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# using indexing and key to get the relevant output score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0munmasker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dana was [MASK] that Mars has no water.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"surprised\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'unmasker' is not defined"
     ]
    }
   ],
   "source": [
    "# using indexing and key to get the relevant output score\n",
    "unmasker(\"Dana was [MASK] that Mars has no water.\",targets=\"surprised\")[0]['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlm_over_df(input_df):\n",
    "    for row in input_df.itterows():\n",
    "        sentence = f\"{s}\".format(s=input_df[\"sentence\"])\n",
    "        verb = f\"{v}\".format(v=input_df[\"verb\"])\n",
    "        mask_fill = unmasker(sentence, targets=verb)\n",
    "        input_df[\"mlm_score\"] = mask_fill[0]['score']\n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Sentence_Masked\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing data for language modeling\n",
    "Take sentences from CommitmentBank, MegaAttitudes, and stimuli from experiment, mask the attitude predicate, and get predicted probability of occurrence for the target verb. Then, calculate from that the surprisal of the verb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This makes the display show more info\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "1. [Read in the three datasets](#Read-in-the-three-datasets)\n",
    "2. [Masking out the correct verb](#Masking-out-the-correct-verb)\n",
    "    1. [Remaining cases](#Remaining-cases)\n",
    "    2. [Proposed Solution](#Proposed-Solution)\n",
    "        1. [Step 1. Create a new column with list of pos tagged verbs from Sentence](#Step-1.-Create-a-new-column-with-list-of-pos-tagged-verbs-from-Sentence)\n",
    "        2. [Step 2. Lemmatize VerbList](#Step-2.-Lemmatize-VerbList)\n",
    "        3. [TROUBLESHOOT NEEDED](#TROUBLESHOOT-NEEDED)\n",
    "    3. [Combine the datafriends together again](#Combine-the-dataframes-together-again)\n",
    "    4. [Mask out the VerbToken from Sentence](#Mask-out-the-VerbToken-from-Sentence)\n",
    "4. [Masked language modeling to estimate surprisal](#Masked-language-modeling-to-estimate-surprisal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the three datasets\n",
    "- Subset the dfs to just the relevant columns: ID, Verb, Sentence\n",
    "- Make sure that the column names are consistent across the tree dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CommitmentBank\n",
    "# raw url: https://raw.githubusercontent.com/khuyen-le/projectivity-factors/master/data/CommitmentBank-All.csv\n",
    "cb = pd.read_csv(\"../../data/CommitmentBank-ALL.csv\")[[\"uID\",\"Verb\",\"Target\"]].drop_duplicates()\n",
    "cb = cb.rename(columns={\"Target\": \"Sentence\",\"uID\":\"ID\"})\n",
    "len(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5026"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MegaVeridicality\n",
    "# raw URL: https://raw.githubusercontent.com/khuyen-le/projectivity-factors/master/data/mega-veridicality-v2.csv\n",
    "mv = pd.read_csv(\"../../data/mega-veridicality-v2.csv\")[[\"verb\",\"frame\",\"voice\",\"sentence\"]].drop_duplicates()\n",
    "mv = mv.rename(columns={\"verb\": \"Verb\", \"sentence\":\"Sentence\"})\n",
    "mv[\"ID\"] = mv[['frame', 'voice']].apply(lambda x: '_'.join(x), axis=1)\n",
    "mv = mv.drop(columns=[\"frame\",\"voice\"])\n",
    "len(mv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Arousal/Valence Study\n",
    "# raw URL: https://raw.githubusercontent.com/khuyen-le/projectivity-factors/master/data/1_sliderprojection/exp1_test-trials.csv\n",
    "vs = pd.read_csv(\"../../data/1_sliderprojection/exp1_test-trials.csv\")[[\"Word\",\"utterance\",\"exp\"]]\n",
    "vs = vs[vs[\"exp\"]==\"stim\"].drop_duplicates().drop(columns={\"exp\"})\n",
    "vs = vs.rename(columns={\"Word\": \"Verb\",\"utterance\":\"Sentence\"})\n",
    "vs[\"ID\"] = \"projection\"\n",
    "len(vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine them together into one df\n",
    "df = pd.concat([cb,mv,vs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6280"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1200 + 5026 + 54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6280"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the correct verb token\n",
    "What we need to do is mask out the correct verb in each of the sentences. We have the correct verb in the Verb column. We can easily use apply() with str.replace() to switch the verb with [MASK]. The problem is that the verbs in the sentences are inflected tokens, while the verbs in Verb are lemmatized.\n",
    "\n",
    "\n",
    "For some of the verbs, we don't need to worry about this problem because there is morphological overlap between the Verb Token and the Verb Lemma. \n",
    "\n",
    "\n",
    "Solution:\n",
    "1. Create a new verb token column\n",
    "2. Regex + literal string interpolation to match works in cases where the Verb matches morphologically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frustratingly, this isn't working\n",
    "# df[\"VerbToken\"] = df['Sentence'].str.extract(fr'({df[\"Verb\"]}\\w*)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find a match in the Sentence column for the verb from the Verb column using a regex re.search() returns a match object, so you have to call .group() to get the string that is matched. In cases where there is no match, a NoneType object is returned and you can't call .group() on that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Token\"] = df.apply(lambda x: re.search(fr'({x[\"Verb\"]}\\w*)',x['Sentence']), axis=1)\n",
    "\n",
    "# In some cases there is nothing captured, it returns a NoneType and causes the code to fail\n",
    "# because NoneType has no method .group()\n",
    "df[\"Token\"] = df[\"Token\"].apply(lambda x: x.group() if x is not None else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask out the VerbToken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonempty = df[~df[\"Token\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-cef8d8788cd3>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nonempty[\"Masked\"] = nonempty.apply(lambda x: x['Sentence'].replace(x[\"Token\"],\"[MASK]\"),axis=1)\n"
     ]
    }
   ],
   "source": [
    "nonempty[\"Masked\"] = nonempty.apply(lambda x: x['Sentence'].replace(x[\"Token\"],\"[MASK]\"),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5711"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nonempty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6280"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5711+569"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6280"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remaining cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "569"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cases where the above solution did not work\n",
    "empty = df[df[\"Token\"].isnull()]\n",
    "len(empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.060509554140127"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(empty)/len(df)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed Solution\n",
    "Overarching: lemmatize Sentence, find the verb lemma that matches the respective Verb column. But we actually need the actual verb token not the lemma, because to replace the correct verb in Sentence with [Mask], we will need to extract the relevant token in order to do a successful str.replace().\n",
    "\n",
    "More concrete:\n",
    "1. Make a new column with POS tag verbs from Sentence\n",
    "2. Lemmatize the verbs from the new column\n",
    "3. Here there be dragons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Create a new column with list of pos tagged verbs from Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verb(sentence):\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "    verbs = []\n",
    "    for i in nltk_tagged:\n",
    "        if 'VB' in i[1]:\n",
    "            verbs.append(i)\n",
    "    return verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-463aecabe8aa>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  empty[\"VerbList\"] = empty[\"Sentence\"].apply(lambda x: get_verb(x))\n"
     ]
    }
   ],
   "source": [
    "empty[\"VerbList\"] = empty[\"Sentence\"].apply(lambda x: get_verb(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Lemmatize VerbList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-b161d5e1aebb>:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  empty[\"VerbListLemmatized\"] = empty[\"VerbList\"].apply(lambda x: lemmatize_from_nltk_tagged_list(x))\n"
     ]
    }
   ],
   "source": [
    "# code from: https://gaurav5430.medium.com/using-nltk-for-lemmatizing-sentences-c1bfff963258\n",
    "\n",
    "# initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def lemmatize_from_nltk_tagged_list(nltk_tagged):\n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return lemmatized_sentence\n",
    "\n",
    "empty[\"VerbListLemmatized\"] = empty[\"VerbList\"].apply(lambda x: lemmatize_from_nltk_tagged_list(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pull just the Verb Token and Lemma from the VerbListLemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-65e54aa3f32b>:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  empty[\"LemmaTokenPair\"] = iter_list\n"
     ]
    }
   ],
   "source": [
    "# solution by brandon papineau\n",
    "check_list = []\n",
    "iter_list = []\n",
    "for index,row in empty.iterrows():\n",
    "    inner_list = []\n",
    "    if row[\"Verb\"] not in check_list:\n",
    "        check_list.append(row[\"Verb\"])\n",
    "    for i in row[\"VerbListLemmatized\"]:\n",
    "        if i in check_list:\n",
    "            lemma = i\n",
    "            locator = row[\"VerbListLemmatized\"].index(i)\n",
    "            tagged = row[\"VerbList\"][locator]\n",
    "            inner_list.append([lemma,tagged])\n",
    "    iter_list.append(inner_list)\n",
    "empty[\"LemmaTokenPair\"] = iter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "569"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(empty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate out the sucessful cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "425"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good = empty[empty.astype(str)[\"LemmaTokenPair\"] != \"[]\"]\n",
    "len(good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-cb81d4b0c469>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  good[\"Token\"] = good['LemmaTokenPair'].apply(lambda x: x[0][1][0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "425"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good[\"Token\"] = good['LemmaTokenPair'].apply(lambda x: x[0][1][0])\n",
    "len(good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask out the verb token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-4e4e0dd71563>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  good[\"Masked\"] = good.apply(lambda x: x['Sentence'].replace(x[\"Token\"],\"[MASK]\"),axis=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "425"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good[\"Masked\"] = good.apply(lambda x: x['Sentence'].replace(x[\"Token\"],\"[MASK]\"),axis=1)\n",
    "len(good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate out the unsucessful cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several cases aren't getting caught because:\n",
    "1. The word isn't correctly tagged as a verb, so not ending up in VerbList in the first place\n",
    "    - example: 'thought' in item 901\n",
    "2. The word isn't lemmatized correctly, so the match with Verb isn't happening\n",
    "    - examples: 'felt'\n",
    "3. Orthographic differences/errors\n",
    "    - examples: 'realize'/'realise', 'facinate' / 'fascinate'\n",
    "4. Cases where the Verb has a particle ---> the majority of cases\n",
    "    - 'flip_out' vs. 'flip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing = empty[empty.astype(str)[\"LemmaTokenPair\"] == \"[]\"]\n",
    "len(missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at cases of Type 4\n",
    "Solution: same as before but try a str.constains or something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_4 = missing.loc[missing[\"Verb\"].str.contains(\"_\")].drop(columns={\"LemmaTokenPair\"})\n",
    "len(type_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_4a = type_4[\"Verb\"].str.split(\"_\",expand=True)\n",
    "type_4 = type_4a.merge(type_4, left_index = True, right_index = True).rename(columns={0:\"VerbSplit\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run BPap's code again\n",
    "check_list = []\n",
    "iter_list = []\n",
    "for index,row in type_4.iterrows():\n",
    "    inner_list = []\n",
    "    if row[\"VerbSplit\"] not in check_list: # search for check on the result of splitting the Verb column\n",
    "        check_list.append(row[\"VerbSplit\"])\n",
    "    for i in row[\"VerbListLemmatized\"]:\n",
    "        if i in check_list:\n",
    "            lemma = i\n",
    "            locator = row[\"VerbListLemmatized\"].index(i)\n",
    "            tagged = row[\"VerbList\"][locator]\n",
    "            inner_list.append([lemma,tagged])\n",
    "    iter_list.append(inner_list)\n",
    "type_4[\"LemmaTokenPair\"] = iter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split up the LemmaTokenPair column into two columns\n",
    "type_4 = type_4.LemmaTokenPair.apply(pd.Series).merge(type_4, left_index = True, right_index = True)\n",
    "len(type_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cases where that worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t4_nonnull = type_4.loc[~type_4[0].isnull()]\n",
    "len(t4_nonnull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "t4_nonnull = t4_nonnull[0].apply(pd.Series).merge(t4_nonnull, left_index = True, right_index = True)\n",
    "t4_nonnull = t4_nonnull.rename(columns={\"0_x\":\"Lemma\"})\n",
    "t4_nonnull[\"Token\"] = t4_nonnull[\"1_x\"].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "t4_nonnull[\"Masked\"] = t4_nonnull.apply(lambda x: x['Sentence'].replace(x[\"Token\"],\"[MASK]\"),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t4_nonnull)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missed cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t4_null = type_4.loc[type_4[0].isnull()]\n",
    "len(t4_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t4_null = t4_null.drop(columns=[0])\n",
    "len(t4_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "t4_null[\"Token\"] = t4_null.apply(lambda x: re.search(fr'({x[\"VerbSplit\"]}\\w*)',x['Sentence']), axis=1)\n",
    "t4_null[\"Token\"] = t4_null[\"Token\"].apply(lambda x: x.group() if x is not None else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "t4_null[\"Masked\"] = t4_null.apply(lambda x: x['Sentence'].replace(x[\"Token\"],\"[MASK]\"),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t4_null)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Everything else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "every_else = missing.loc[~missing[\"Verb\"].str.contains(\"_\")].drop(columns={\"LemmaTokenPair\"})\n",
    "len(every_else)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "every_else = every_else[[\"ID\",\"Verb\",\"Sentence\"]]\n",
    "every_else[\"Token\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orthographic differences/errors\n",
    "every_else[\"Verb\"].loc[every_else[\"Verb\"]==\"facinate\"] = \"fascinate\"\n",
    "every_else[\"Verb\"].loc[every_else[\"Verb\"]==\"realize\"] = \"realise\"\n",
    "\n",
    "every_else[\"Token\"] = every_else.apply(lambda x: re.search(fr'({x[\"Verb\"]}\\w*)',x['Sentence']), axis=1)\n",
    "every_else[\"Token\"] = every_else[\"Token\"].apply(lambda x: x.group() if x is not None else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize all the irregular conjugations\n",
    "every_else[\"Token\"].loc[every_else[\"Verb\"]==\"understand\"] = \"understood\"\n",
    "every_else[\"Token\"].loc[every_else[\"Verb\"]==\"feel\"] = \"felt\"\n",
    "every_else[\"Token\"].loc[every_else[\"Verb\"]==\"think\"] = \"thought\"\n",
    "every_else[\"Token\"].loc[every_else[\"Verb\"]==\"hope\"] = \"hoping\"\n",
    "every_else[\"Token\"].loc[every_else[\"Verb\"]==\"see\"] = \"saw\"\n",
    "every_else[\"Token\"].loc[every_else[\"Verb\"]==\"spellbind\"] = \"spellbound\"\n",
    "every_else[\"Token\"].loc[every_else[\"Verb\"]==\"sing\"] = \"sung\"\n",
    "every_else[\"Token\"].loc[every_else[\"Verb\"]==\"swear\"] = \"swore\"\n",
    "every_else[\"Token\"].loc[every_else[\"Verb\"]==\"bear\"] = \"borne\"\n",
    "every_else[\"Token\"].loc[every_else[\"Verb\"]==\"choose\"] = \"chosen\"\n",
    "every_else[\"Token\"].loc[every_else[\"Verb\"]==\"undertake\"] = \"undertook\"\n",
    "every_else[\"Token\"].loc[every_else[\"Verb\"]==\"uphold\"] = \"upheld\"\n",
    "every_else[\"Token\"].loc[every_else[\"Verb\"]==\"satisfy\"] = \"satisfied\"\n",
    "every_else[\"Token\"].loc[every_else[\"Verb\"]==\"teach\"] = \"taught\"\n",
    "every_else[\"Token\"].loc[every_else[\"Verb\"]==\"foretell\"] = \"foretold\"\n",
    "every_else[\"Token\"].loc[every_else[\"Verb\"]==\"curse\"] = \"curst\"\n",
    "every_else[\"Token\"].loc[every_else[\"Verb\"]==\"send\"] = \"sent\"\n",
    "every_else[\"Token\"].loc[every_else[\"Verb\"]==\"teach\"] = \"taught\"\n",
    "every_else[\"Token\"].loc[every_else[\"Verb\"]==\"weep\"] = \"wept\"\n",
    "every_else[\"Token\"].loc[every_else[\"Verb\"]==\"fight\"] = \"faught\"\n",
    "every_else[\"Token\"].loc[every_else[\"Verb\"]==\"forbid\"] = \"forbade\"\n",
    "len(every_else)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MASK OUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "every_else[\"Masked\"] = every_else.apply(lambda x: x['Sentence'].replace(x[\"Token\"],\"[MASK]\"),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(every_else)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine the dataframes together again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nonempty: 5711\n",
      "good: 425\n",
      "t4_nonnull: 82\n",
      "t4_null: 19\n",
      "every_else: 43\n",
      "total: 6280\n"
     ]
    }
   ],
   "source": [
    "print(f\"nonempty: {len(nonempty)}\")\n",
    "print(f\"good: {len(good)}\")\n",
    "print(f\"t4_nonnull: {len(t4_nonnull)}\")\n",
    "print(f\"t4_null: {len(t4_null)}\")\n",
    "print(f\"every_else: {len(every_else)}\")\n",
    "print(f\"total: {len(nonempty) + len(good) + len(t4_nonnull) + len(t4_null) + len(every_else)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonempty = nonempty[[\"ID\",\"Verb\",\"Sentence\",\"Masked\",\"Token\"]]\n",
    "good = good[[\"ID\",\"Verb\",\"Sentence\",\"Masked\",\"Token\"]]\n",
    "t4_nonnull = t4_nonnull[[\"ID\",\"Verb\",\"Sentence\",\"Masked\",\"Token\"]]\n",
    "t4_null = t4_null[[\"ID\",\"Verb\",\"Sentence\",\"Masked\",\"Token\"]]\n",
    "every_else = every_else[[\"ID\",\"Verb\",\"Sentence\",\"Masked\",\"Token\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6280"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nonempty) + len(good) + len(t4_nonnull) + len(t4_null) + len(every_else)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6280"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = pd.concat([nonempty,good,t4_nonnull,t4_null,every_else])\n",
    "len(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.to_csv(\"../../data/data_for_lm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
